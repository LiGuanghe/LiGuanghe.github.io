<- [12d[LOG][ch3]记录任务过程 · Issue #71 · AIHackers/DeepLearning101-002](https://github.com/AIHackers/DeepLearning101-002/issues/71)

# ch4：神经网络语言模型

## 构建神经网络语言模型
- 了解神经网络语言模型基本原理及其优势，尝试用矩阵的形式表达词汇，构建模型。
- Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.
* 理解神经网络的矩阵表示形式
* 了解神经网络语言模型的优势
* 理解 Word embedding 的概念（中文可解释为 “词嵌入”，Word embedding 作用是将源数据映射到另一个空间）


本章代码框架见：[DeepLearning101-002/ch4/code at master · AIHackers/DeepLearning101-002](https://github.com/AIHackers/DeepLearning101-002/tree/master/ch4/code)
## 更深入地理解神经网络
- [x] 1122 视频1
- 反向传播算法（Back propagation）是神经网络的一个重要算法，其基本思想和梯度下降法一致，那在神经网络中，反向传播算法具体是怎么实现？ 
    -  [Backpropagation.pdf - J.G. Makin](https://inst.eecs.berkeley.edu/~cs182/sp06/notes/backprop.pdf)
    - 推荐阅读 Michael Nielsen 的电子书第2章 [Neural Networks and Deep Learning - Chapter 2](http://neuralnetworksanddeeplearning.com/chap2.html) , 有时间的话把数学公式自己推导一遍，理解更透彻。
- [x] 1123 任务一：基于矩阵乘法实现单隐层神经网络
- 基于矩阵乘法用 Tensorflow 实现 ch3 作业的单隐层神经网络。
- 提示：Tensorflow 中有许多关于矩阵运算的命令，可以参考使用。
- TensorFlow 矩阵运算： [Matrix Math Functions  |  TensorFlow](https://www.tensorflow.org/api_guides/python/math_ops#Matrix_Math_Functions)

## 神经网络的表达能力

- [x] 1123 视频2 
```
- 节点越多, 神经网络逼近尽力越强
- 多层少节点, 效果比单隐层好
- 太宽太深又会有过拟合的问题, 连噪音也拟合了, 新数据出来效果差
- 解决过拟合: 
    - 把数据分成三份, 大部分训练集, 一部分测试集, 一部分验证集. 训练集拟合, 验证集看效果, 测试集最终看效果, 只用一次. 
```
- 虽然目前深度方向的优势，尚无理论上的定论，但实际效果很好。但要注意，深度或宽度方向上的扩张，都不能没有限制，否则会出现过拟合的问题。
- [Universal approximation theorem - Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem)

## 3. 矩阵符号表示神经网络 revisit
- [x] 1124 视频3
    - 行向量和列向量的表达形式，是学习神经网络的必备技能。如果你对矩阵乘法理解不足，建议你查阅《线性代数》的相关资料，补习一下。

    - W_{ij} 一般表示矩阵里的第 i 行第 j 个元素，但视频中把输入从列向量转换成行向量时，未对权重矩阵作转置表示，你要注意这点哟
```
- 一般用列向量表达输入
- TensorFlow 用航向了表达输入, 放在左边. 
- 列向量转成行向量?
- 增加一层样本, 就是增加一行
- 矩阵乘法 矩阵×向量,= 向量? 矩阵?
```

## 4.神经网络语言模型

- [x] 1123 视频4

```
- n-gram 模型 死记硬背, 无法迁移知识
- 找出很像的词 互相迁移他们的上下文和知识
- 把输出转成概率, softmax
    - 输出都> 0
    - 符合概率分布
    - 概率加起来=1
- 输入
    - 词下文的概率, 之间添加一层 k, 添加一层就减少了产生
    - Word embedding 词向量
    - 词的权重有意义, 但只是1 的时候有意义, 其他都×0, 矩阵乘法在这次比较浪费
    - eval() 
    - 定义1, 然后0的忽略掉
    - 输入是横向量 
```
神经网络语言模型与传统统计语言模型有什么优势呢？现在你一定清楚了。  

Word embedding 概念

- [Deep Learning in NLP （一）词向量和语言模型 – licstar的博客](http://licstar.net/archives/328)  
- [Deep Learning for Natural Language Processing.pdf - stanford.edu ](http://cs224d.stanford.edu/lectures/CS224d-Lecture2.pdf)  

Softmax 函数可以助你构建神经网络语言模型

 [Softmax function - Wikipedia](https://en.wikipedia.org/wiki/Softmax_function)


- 神经网络的两种矩阵表示形式
  - 行向量
  - 列向量
- 神经网络构建语言模型的优势
  - 从深度和广度来提高自身知识的迁移性
- 搭建神经网络构建语言模型
  - word embedding
  - softmax

课程仓库有关于作业的框架参考，大家可以借此减少认知负荷，关注核心内容。本章代码框架见：[DeepLearning101-002/ch4/code at master · AIHackers/DeepLearning101-002](https://github.com/AIHackers/DeepLearning101-002/tree/master/ch4/code)

熟知神经网络相关的数学原理，能助你在课程内外阅读文献或编写代码时更顺利。当然，最关键的是一定要多写代码，做中学，才能加深你的理解。



## 任务二：使用 tf.layers.dense 替换矩阵乘法

- 在课程仓库中找到基础代码 [DeepLearning101-002/tf_matrix.ipynb at master · AIHackers/DeepLearning101-002](https://github.com/AIHackers/DeepLearning101-002/blob/master/ch4/code/tf_matrix.ipynb)
- 将列向量表示形式改成行向量表示形式
- 使用 tf.layers.dense 替换原有的矩阵乘法

对 tf.layer.dense 有疑问？阅读这篇文档，能助你更好理解：[Module: tf.layers |  TensorFlow](https://www.tensorflow.org/api_docs/python/tf/layers)


## 任务三：构建 word embedding
根据基本的框架程序：  

- 使用你的语料库进行训练，构建 NN language model
- 完成 3 个名词各自最相近的 Top 10 个词的检索


为什么取这个矩阵的某一行，和这个词出现的时候做矩阵乘法，结果是一样的呢？

这个小问题，能考核你是否真正理解了 Tensorflow 中 Word embedding 的工作原理。  

如果你真正理解矩阵乘法的原理，问题答案一目了解。

1123- 1124 150 min
